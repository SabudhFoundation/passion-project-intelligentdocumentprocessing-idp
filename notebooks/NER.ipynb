{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35432,"status":"ok","timestamp":1719246256585,"user":{"displayName":"Ishleen Kaur","userId":"04542430752193049796"},"user_tz":-330},"id":"cJDDJXK0NBTD","outputId":"eeb309c7-b7d0-4307-aa3c-ffccff0bd59d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting en-core-web-sm==3.7.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy\u003c3.8.0,\u003e=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n","Requirement already satisfied: spacy-legacy\u003c3.1.0,\u003e=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (3.0.12)\n","Requirement already satisfied: spacy-loggers\u003c2.0.0,\u003e=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (1.0.5)\n","Requirement already satisfied: murmurhash\u003c1.1.0,\u003e=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (1.0.10)\n","Requirement already satisfied: cymem\u003c2.1.0,\u003e=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (2.0.8)\n","Requirement already satisfied: preshed\u003c3.1.0,\u003e=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (3.0.9)\n","Requirement already satisfied: thinc\u003c8.3.0,\u003e=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (8.2.4)\n","Requirement already satisfied: wasabi\u003c1.2.0,\u003e=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (1.1.3)\n","Requirement already satisfied: srsly\u003c3.0.0,\u003e=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (2.4.8)\n","Requirement already satisfied: catalogue\u003c2.1.0,\u003e=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (2.0.10)\n","Requirement already satisfied: weasel\u003c0.5.0,\u003e=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (0.4.1)\n","Requirement already satisfied: typer\u003c1.0.0,\u003e=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (0.12.3)\n","Requirement already satisfied: tqdm\u003c5.0.0,\u003e=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (4.66.4)\n","Requirement already satisfied: requests\u003c3.0.0,\u003e=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,\u003c3.0.0,\u003e=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (2.7.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (67.7.2)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (24.1)\n","Requirement already satisfied: langcodes\u003c4.0.0,\u003e=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (3.4.0)\n","Requirement already satisfied: numpy\u003e=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (1.25.2)\n","Requirement already satisfied: language-data\u003e=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes\u003c4.0.0,\u003e=3.2.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (1.2.0)\n","Requirement already satisfied: annotated-types\u003e=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,\u003c3.0.0,\u003e=1.7.4-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,\u003c3.0.0,\u003e=1.7.4-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (2.18.4)\n","Requirement already satisfied: typing-extensions\u003e=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,\u003c3.0.0,\u003e=1.7.4-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (4.12.2)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (3.7)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (2.0.7)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (2024.6.2)\n","Requirement already satisfied: blis\u003c0.8.0,\u003e=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc\u003c8.3.0,\u003e=8.2.2-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (0.7.11)\n","Requirement already satisfied: confection\u003c1.0.0,\u003e=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc\u003c8.3.0,\u003e=8.2.2-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (0.1.5)\n","Requirement already satisfied: click\u003e=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer\u003c1.0.0,\u003e=0.3.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (8.1.7)\n","Requirement already satisfied: shellingham\u003e=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer\u003c1.0.0,\u003e=0.3.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (1.5.4)\n","Requirement already satisfied: rich\u003e=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer\u003c1.0.0,\u003e=0.3.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (13.7.1)\n","Requirement already satisfied: cloudpathlib\u003c1.0.0,\u003e=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel\u003c0.5.0,\u003e=0.1.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (0.18.1)\n","Requirement already satisfied: smart-open\u003c8.0.0,\u003e=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel\u003c0.5.0,\u003e=0.1.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (7.0.4)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (2.1.5)\n","Requirement already satisfied: marisa-trie\u003e=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data\u003e=1.2-\u003elangcodes\u003c4.0.0,\u003e=3.2.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (1.2.0)\n","Requirement already satisfied: markdown-it-py\u003e=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich\u003e=10.11.0-\u003etyper\u003c1.0.0,\u003e=0.3.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (3.0.0)\n","Requirement already satisfied: pygments\u003c3.0.0,\u003e=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich\u003e=10.11.0-\u003etyper\u003c1.0.0,\u003e=0.3.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (2.16.1)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open\u003c8.0.0,\u003e=5.2.1-\u003eweasel\u003c0.5.0,\u003e=0.1.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (1.14.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py\u003e=2.2.0-\u003erich\u003e=10.11.0-\u003etyper\u003c1.0.0,\u003e=0.3.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-sm==3.7.1) (0.1.2)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}],"source":["!python -m spacy download en_core_web_sm"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6294,"status":"ok","timestamp":1719246262876,"user":{"displayName":"Ishleen Kaur","userId":"04542430752193049796"},"user_tz":-330},"id":"UdCu2Ak6NCbS","outputId":"89a6ce32-6c97-4822-86ab-0bb285d9f0a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Apple is looking at buying U.K. startup for $1 billion\n","\n","Named Entities:\n","Apple ORG\n","U.K. GPE\n","$1 billion MONEY\n"]}],"source":["import spacy\n","from spacy.lang.en.examples import sentences\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","doc = nlp(sentences[0])\n","print(doc.text)\n","print(\"\\nNamed Entities:\")\n","for ent in doc.ents:\n","    print(ent.text, ent.label_)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40321,"status":"ok","timestamp":1719246303177,"user":{"displayName":"Ishleen Kaur","userId":"04542430752193049796"},"user_tz":-330},"id":"IRFkZIdLNLH0","outputId":"62df3d50-bd3d-4e11-9bcb-3426d6ed2a91"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting en-core-web-lg==3.7.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy\u003c3.8.0,\u003e=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.7.1) (3.7.5)\n","Requirement already satisfied: spacy-legacy\u003c3.1.0,\u003e=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (3.0.12)\n","Requirement already satisfied: spacy-loggers\u003c2.0.0,\u003e=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (1.0.5)\n","Requirement already satisfied: murmurhash\u003c1.1.0,\u003e=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (1.0.10)\n","Requirement already satisfied: cymem\u003c2.1.0,\u003e=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (2.0.8)\n","Requirement already satisfied: preshed\u003c3.1.0,\u003e=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (3.0.9)\n","Requirement already satisfied: thinc\u003c8.3.0,\u003e=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (8.2.4)\n","Requirement already satisfied: wasabi\u003c1.2.0,\u003e=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (1.1.3)\n","Requirement already satisfied: srsly\u003c3.0.0,\u003e=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (2.4.8)\n","Requirement already satisfied: catalogue\u003c2.1.0,\u003e=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (2.0.10)\n","Requirement already satisfied: weasel\u003c0.5.0,\u003e=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (0.4.1)\n","Requirement already satisfied: typer\u003c1.0.0,\u003e=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (0.12.3)\n","Requirement already satisfied: tqdm\u003c5.0.0,\u003e=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (4.66.4)\n","Requirement already satisfied: requests\u003c3.0.0,\u003e=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,\u003c3.0.0,\u003e=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (2.7.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (67.7.2)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (24.1)\n","Requirement already satisfied: langcodes\u003c4.0.0,\u003e=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (3.4.0)\n","Requirement already satisfied: numpy\u003e=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (1.25.2)\n","Requirement already satisfied: language-data\u003e=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes\u003c4.0.0,\u003e=3.2.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (1.2.0)\n","Requirement already satisfied: annotated-types\u003e=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,\u003c3.0.0,\u003e=1.7.4-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,\u003c3.0.0,\u003e=1.7.4-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (2.18.4)\n","Requirement already satisfied: typing-extensions\u003e=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,\u003c3.0.0,\u003e=1.7.4-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (4.12.2)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (3.7)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (2.0.7)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (2024.6.2)\n","Requirement already satisfied: blis\u003c0.8.0,\u003e=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc\u003c8.3.0,\u003e=8.2.2-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (0.7.11)\n","Requirement already satisfied: confection\u003c1.0.0,\u003e=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc\u003c8.3.0,\u003e=8.2.2-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (0.1.5)\n","Requirement already satisfied: click\u003e=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer\u003c1.0.0,\u003e=0.3.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (8.1.7)\n","Requirement already satisfied: shellingham\u003e=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer\u003c1.0.0,\u003e=0.3.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (1.5.4)\n","Requirement already satisfied: rich\u003e=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer\u003c1.0.0,\u003e=0.3.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (13.7.1)\n","Requirement already satisfied: cloudpathlib\u003c1.0.0,\u003e=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel\u003c0.5.0,\u003e=0.1.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (0.18.1)\n","Requirement already satisfied: smart-open\u003c8.0.0,\u003e=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel\u003c0.5.0,\u003e=0.1.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (7.0.4)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (2.1.5)\n","Requirement already satisfied: marisa-trie\u003e=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data\u003e=1.2-\u003elangcodes\u003c4.0.0,\u003e=3.2.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (1.2.0)\n","Requirement already satisfied: markdown-it-py\u003e=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich\u003e=10.11.0-\u003etyper\u003c1.0.0,\u003e=0.3.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (3.0.0)\n","Requirement already satisfied: pygments\u003c3.0.0,\u003e=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich\u003e=10.11.0-\u003etyper\u003c1.0.0,\u003e=0.3.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (2.16.1)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open\u003c8.0.0,\u003e=5.2.1-\u003eweasel\u003c0.5.0,\u003e=0.1.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (1.14.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py\u003e=2.2.0-\u003erich\u003e=10.11.0-\u003etyper\u003c1.0.0,\u003e=0.3.0-\u003espacy\u003c3.8.0,\u003e=3.7.2-\u003een-core-web-lg==3.7.1) (0.1.2)\n","Installing collected packages: en-core-web-lg\n","Successfully installed en-core-web-lg-3.7.1\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_lg')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}],"source":["!python -m spacy download en_core_web_lg"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2262,"status":"ok","timestamp":1719246305419,"user":{"displayName":"Ishleen Kaur","userId":"04542430752193049796"},"user_tz":-330},"id":"Zm8tVvYcNeEJ","outputId":"035a59ae-396f-4fa1-b27f-99f12b05f8f2"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n","  warnings.warn(Warnings.W111)\n"]},{"name":"stdout","output_type":"stream","text":["Apple is looking at buying U.K. startup for $1 billion\n","\n","Named Entities:\n","Apple ORG\n","U.K. GPE\n","$1 billion MONEY\n"]}],"source":["import spacy\n","from spacy.lang.en.examples import sentences\n","\n","nlp = spacy.load(\"en_core_web_lg\")\n","doc = nlp(sentences[0])\n","print(doc.text)\n","print(\"\\nNamed Entities:\")\n","for ent in doc.ents:\n","    print(ent.text, ent.label_)"]},{"cell_type":"markdown","metadata":{"id":"RviuJXicc8eN"},"source":["1. Size:\n","  Small (sm) Model:\n","  Typically smaller in size compared to large models.\n","\n","  Large (lg) Model:\n","  Larger in size compared to small models.\n","\n","2. Capabilities:\n","  Small (sm) Model:\n","  Provides basic linguistic annotations and capabilities necessary for many general-purpose NLP tasks.\n","\n","  Large (lg) Model:\n","  Offers more advanced linguistic capabilities and higher accuracy due to its larger training data and vocabulary.\n","\n","3. Performance:\n","  Small (sm) Model:\n","  Faster processing speed and lower memory compared to large models.\n","\n","  Large (lg) Model:\n","  May require more computational resources (CPU and memory) due to its larger size and more sophisticated processing.\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39201,"status":"ok","timestamp":1719246374608,"user":{"displayName":"Ishleen Kaur","userId":"04542430752193049796"},"user_tz":-330},"id":"hxMGbvdhgoIW","outputId":"bdd4db8c-5f56-4ad9-9d37-090d2776afa1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/huggingface/transformers.git\n","  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-0vwfwr2t\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-0vwfwr2t\n","  Resolved https://github.com/huggingface/transformers.git to commit 3a49ebe0d893ec7ada62b5b06c5394b90288c097\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy\u003e=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.25.2)\n","Requirement already satisfied: scikit-learn\u003e=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (3.15.1)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (0.23.4)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (24.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (2.31.0)\n","Requirement already satisfied: tokenizers\u003c0.20,\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (0.19.1)\n","Requirement already satisfied: safetensors\u003e=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (0.4.3)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.42.0.dev0) (4.66.4)\n","Requirement already satisfied: fsspec\u003e=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.23.2-\u003etransformers==4.42.0.dev0) (2023.6.0)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.23.2-\u003etransformers==4.42.0.dev0) (4.12.2)\n","Requirement already satisfied: scipy\u003e=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn\u003e=0.21.3-\u003eseqeval) (1.11.4)\n","Requirement already satisfied: joblib\u003e=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn\u003e=0.21.3-\u003eseqeval) (1.4.2)\n","Requirement already satisfied: threadpoolctl\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn\u003e=0.21.3-\u003eseqeval) (3.5.0)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers==4.42.0.dev0) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers==4.42.0.dev0) (3.7)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers==4.42.0.dev0) (2.0.7)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers==4.42.0.dev0) (2024.6.2)\n","Building wheels for collected packages: seqeval, transformers\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=30ffb3d1098e26a20ef9c39a08ffb7f54ef21c797e14b0c3bdbf8bdd69ab3434\n","  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n","  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.42.0.dev0-py3-none-any.whl size=9233039 sha256=b426c58b7356f7cca96ac65b65c67657af9d5e228fd6e67299749be7921455a7\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-hj2sat_i/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n","Successfully built seqeval transformers\n","Installing collected packages: seqeval, transformers\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.41.2\n","    Uninstalling transformers-4.41.2:\n","      Successfully uninstalled transformers-4.41.2\n","Successfully installed seqeval-1.2.2 transformers-4.42.0.dev0\n"]}],"source":["!pip install seqeval git+https://github.com/huggingface/transformers.git"]},{"cell_type":"markdown","metadata":{"id":"iVM-WaK9j4sO"},"source":["Using LUKE on CoNLL-2003 Using Hugging Face Transformers"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2556,"status":"ok","timestamp":1719246377912,"user":{"displayName":"Ishleen Kaur","userId":"04542430752193049796"},"user_tz":-330},"id":"C_xfqCr2gx6P"},"outputs":[],"source":["import unicodedata\n","\n","import numpy as np\n","import seqeval.metrics\n","import spacy\n","import torch\n","from tqdm import tqdm, trange\n","from transformers import LukeTokenizer, LukeForEntitySpanClassification"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1719246377912,"user":{"displayName":"Ishleen Kaur","userId":"04542430752193049796"},"user_tz":-330},"id":"aoiYhwWbg1ze","outputId":"babb8265-dd2d-4a54-971f-0bef8e385261"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-06-24 16:26:08--  https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testb\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 748096 (731K) [text/plain]\n","Saving to: ‘eng.testb’\n","\n","\reng.testb             0%[                    ]       0  --.-KB/s               \reng.testb           100%[===================\u003e] 730.56K  --.-KB/s    in 0.03s   \n","\n","2024-06-24 16:26:08 (21.4 MB/s) - ‘eng.testb’ saved [748096/748096]\n","\n"]}],"source":["!wget https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testb"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":205},"id":"j0X9tGk4g5nq"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0d1bef4d76b34cf59494f7befc9d3c0e","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/877 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fa04f220c5b348d78ffd9d9005ba5814","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/2.24G [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at studio-ousia/luke-large-finetuned-conll-2003 were not used when initializing LukeForEntitySpanClassification: ['luke.embeddings.position_ids']\n","- This IS expected if you are initializing LukeForEntitySpanClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing LukeForEntitySpanClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"ename":"RuntimeError","evalue":"Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-11-62a46ac17601\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 4\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLukeForEntitySpanClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"studio-ousia/luke-large-finetuned-conll-2003\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2794\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2795\u001b[0m                 )\n\u001b[0;32m-\u003e 2796\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2798\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1173\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 779\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 779\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 779\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 804\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1157\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                     )\n\u001b[0;32m-\u003e 1159\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1160\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 293\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"]}],"source":["# Load the model checkpoint\n","model = LukeForEntitySpanClassification.from_pretrained(\"studio-ousia/luke-large-finetuned-conll-2003\")\n","model.eval()\n","model.to(\"cuda\")\n","\n","# Load the tokenizer\n","tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-large-finetuned-conll-2003\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7cKECX4Cg-h9"},"outputs":[],"source":["def load_documents(dataset_file):\n","    documents = []\n","    words = []\n","    labels = []\n","    sentence_boundaries = []\n","    with open(dataset_file) as f:\n","        for line in f:\n","            line = line.rstrip()\n","            if line.startswith(\"-DOCSTART\"):\n","                if words:\n","                    documents.append(dict(\n","                        words=words,\n","                        labels=labels,\n","                        sentence_boundaries=sentence_boundaries\n","                    ))\n","                    words = []\n","                    labels = []\n","                    sentence_boundaries = []\n","                continue\n","\n","            if not line:\n","                if not sentence_boundaries or len(words) != sentence_boundaries[-1]:\n","                    sentence_boundaries.append(len(words))\n","            else:\n","                items = line.split(\" \")\n","                words.append(items[0])\n","                labels.append(items[-1])\n","\n","    if words:\n","        documents.append(dict(\n","            words=words,\n","            labels=labels,\n","            sentence_boundaries=sentence_boundaries\n","        ))\n","\n","    return documents\n","\n","\n","def load_examples(documents):\n","    examples = []\n","    max_token_length = 510\n","    max_mention_length = 30\n","\n","    for document in tqdm(documents):\n","        words = document[\"words\"]\n","        subword_lengths = [len(tokenizer.tokenize(w)) for w in words]\n","        total_subword_length = sum(subword_lengths)\n","        sentence_boundaries = document[\"sentence_boundaries\"]\n","\n","        for i in range(len(sentence_boundaries) - 1):\n","            sentence_start, sentence_end = sentence_boundaries[i:i+2]\n","            if total_subword_length \u003c= max_token_length:\n","                # if the total sequence length of the document is shorter than the\n","                # maximum token length, we simply use all words to build the sequence\n","                context_start = 0\n","                context_end = len(words)\n","            else:\n","                # if the total sequence length is longer than the maximum length, we add\n","                # the surrounding words of the target sentence　to the sequence until it\n","                # reaches the maximum length\n","                context_start = sentence_start\n","                context_end = sentence_end\n","                cur_length = sum(subword_lengths[context_start:context_end])\n","                while True:\n","                    if context_start \u003e 0:\n","                        if cur_length + subword_lengths[context_start - 1] \u003c= max_token_length:\n","                            cur_length += subword_lengths[context_start - 1]\n","                            context_start -= 1\n","                        else:\n","                            break\n","                    if context_end \u003c len(words):\n","                        if cur_length + subword_lengths[context_end] \u003c= max_token_length:\n","                            cur_length += subword_lengths[context_end]\n","                            context_end += 1\n","                        else:\n","                            break\n","\n","            text = \"\"\n","            for word in words[context_start:sentence_start]:\n","                if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n","                    text = text.rstrip()\n","                text += word\n","                text += \" \"\n","\n","            sentence_words = words[sentence_start:sentence_end]\n","            sentence_subword_lengths = subword_lengths[sentence_start:sentence_end]\n","\n","            word_start_char_positions = []\n","            word_end_char_positions = []\n","            for word in sentence_words:\n","                if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n","                    text = text.rstrip()\n","                word_start_char_positions.append(len(text))\n","                text += word\n","                word_end_char_positions.append(len(text))\n","                text += \" \"\n","\n","            for word in words[sentence_end:context_end]:\n","                if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n","                    text = text.rstrip()\n","                text += word\n","                text += \" \"\n","            text = text.rstrip()\n","\n","            entity_spans = []\n","            original_word_spans = []\n","            for word_start in range(len(sentence_words)):\n","                for word_end in range(word_start, len(sentence_words)):\n","                    if sum(sentence_subword_lengths[word_start:word_end]) \u003c= max_mention_length:\n","                        entity_spans.append(\n","                            (word_start_char_positions[word_start], word_end_char_positions[word_end])\n","                        )\n","                        original_word_spans.append(\n","                            (word_start, word_end + 1)\n","                        )\n","\n","            examples.append(dict(\n","                text=text,\n","                words=sentence_words,\n","                entity_spans=entity_spans,\n","                original_word_spans=original_word_spans,\n","            ))\n","\n","    return examples\n","\n","\n","def is_punctuation(char):\n","    cp = ord(char)\n","    if (cp \u003e= 33 and cp \u003c= 47) or (cp \u003e= 58 and cp \u003c= 64) or (cp \u003e= 91 and cp \u003c= 96) or (cp \u003e= 123 and cp \u003c= 126):\n","        return True\n","    cat = unicodedata.category(char)\n","    if cat.startswith(\"P\"):\n","        return True\n","    return False\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"WLIUME_ghEgG"},"outputs":[],"source":["test_documents = load_documents(\"eng.testb\")\n","test_examples = load_examples(test_documents)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"cdMLAMKKhHAd"},"outputs":[],"source":["batch_size = 2\n","all_logits = []\n","\n","for batch_start_idx in trange(0, len(test_examples), batch_size):\n","    batch_examples = test_examples[batch_start_idx:batch_start_idx + batch_size]\n","    texts = [example[\"text\"] for example in batch_examples]\n","    entity_spans = [example[\"entity_spans\"] for example in batch_examples]\n","\n","    inputs = tokenizer(texts, entity_spans=entity_spans, return_tensors=\"pt\", padding=True)\n","    inputs = inputs.to(\"cuda\")\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","    all_logits.extend(outputs.logits.tolist())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"bIUZ9CCZhJ1F"},"outputs":[],"source":["final_labels = [label for document in test_documents for label in document[\"labels\"]]\n","\n","final_predictions = []\n","for example_index, example in enumerate(test_examples):\n","    logits = all_logits[example_index]\n","    max_logits = np.max(logits, axis=1)\n","    max_indices = np.argmax(logits, axis=1)\n","    original_spans = example[\"original_word_spans\"]\n","    predictions = []\n","    for logit, index, span in zip(max_logits, max_indices, original_spans):\n","        if index != 0:  # the span is not NIL\n","            predictions.append((logit, span, model.config.id2label[index]))\n","\n","    # construct an IOB2 label sequence\n","    predicted_sequence = [\"O\"] * len(example[\"words\"])\n","    for _, span, label in sorted(predictions, key=lambda o: o[0], reverse=True):\n","        if all([o == \"O\" for o in predicted_sequence[span[0] : span[1]]]):\n","            predicted_sequence[span[0]] = \"B-\" + label\n","            if span[1] - span[0] \u003e 1:\n","                predicted_sequence[span[0] + 1 : span[1]] = [\"I-\" + label] * (span[1] - span[0] - 1)\n","\n","    final_predictions += predicted_sequence\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"l9vgxS37hRlI"},"outputs":[],"source":["print(seqeval.metrics.classification_report([final_labels], [final_predictions], digits=4))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"BTYODdeKhaDC"},"outputs":[],"source":["text = \"Star Wars is a film written and directed by George Lucas\"\n","nlp = spacy.load(\"en_core_web_sm\")\n","doc = nlp(text)\n","\n","entity_spans = []\n","original_word_spans = []\n","for token_start in doc:\n","    for token_end in doc[token_start.i:]:\n","        entity_spans.append((token_start.idx, token_end.idx + len(token_end)))\n","        original_word_spans.append((token_start.i, token_end.i + 1))\n","\n","inputs = tokenizer(text, entity_spans=entity_spans, return_tensors=\"pt\", padding=True)\n","inputs = inputs.to(\"cuda\")\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","\n","logits = outputs.logits\n","max_logits, max_indices = logits[0].max(dim=1)\n","\n","predictions = []\n","for logit, index, span in zip(max_logits, max_indices, original_word_spans):\n","    if index != 0:  # the span is not NIL\n","        predictions.append((logit, span, model.config.id2label[int(index)]))\n","\n","# construct an IOB2 label sequence\n","predicted_sequence = [\"O\"] * len(doc)\n","for _, span, label in sorted(predictions, key=lambda o: o[0], reverse=True):\n","    if all([o == \"O\" for o in predicted_sequence[span[0] : span[1]]]):\n","        predicted_sequence[span[0]] = \"B-\" + label\n","        if span[1] - span[0] \u003e 1:\n","            predicted_sequence[span[0] + 1 : span[1]] = [\"I-\" + label] * (span[1] - span[0] - 1)\n","\n","for token, label in zip(doc, predicted_sequence):\n","    print(token, label)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"YU1jirA4wgAI"},"outputs":[],"source":["import unicodedata\n","import numpy as np\n","import seqeval.metrics\n","import spacy\n","import torch\n","from tqdm import tqdm, trange\n","from transformers import LukeTokenizer, LukeForEntitySpanClassification\n","\n","# Load the model checkpoint\n","model = LukeForEntitySpanClassification.from_pretrained(\"studio-ousia/luke-large-finetuned-conll-2003\")\n","model.eval()\n","model.to(\"cpu\")\n","\n","# Load the tokenizer\n","tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-large-finetuned-conll-2003\")\n","\n","def load_documents(dataset_file):\n","    documents = []\n","    words = []\n","    labels = []\n","    sentence_boundaries = []\n","    with open(dataset_file) as f:\n","        for line in f:\n","            line = line.rstrip()\n","            if line.startswith(\"-DOCSTART\"):\n","                if words:\n","                    documents.append(dict(\n","                        words=words,\n","                        labels=labels,\n","                        sentence_boundaries=sentence_boundaries\n","                    ))\n","                    words = []\n","                    labels = []\n","                    sentence_boundaries = []\n","                continue\n","\n","            if not line:\n","                if not sentence_boundaries or len(words) != sentence_boundaries[-1]:\n","                    sentence_boundaries.append(len(words))\n","            else:\n","                items = line.split(\" \")\n","                words.append(items[0])\n","                labels.append(items[-1])\n","\n","    if words:\n","        documents.append(dict(\n","            words=words,\n","            labels=labels,\n","            sentence_boundaries=sentence_boundaries\n","        ))\n","\n","    return documents\n","\n","def load_examples(documents):\n","    examples = []\n","    max_token_length = 510\n","    max_mention_length = 30\n","\n","    for document in tqdm(documents):\n","        words = document[\"words\"]\n","        subword_lengths = [len(tokenizer.tokenize(w)) for w in words]\n","        total_subword_length = sum(subword_lengths)\n","        sentence_boundaries = document[\"sentence_boundaries\"]\n","\n","        for i in range(len(sentence_boundaries) - 1):\n","            sentence_start, sentence_end = sentence_boundaries[i:i+2]\n","            if total_subword_length \u003c= max_token_length:\n","                context_start = 0\n","                context_end = len(words)\n","            else:\n","                context_start = sentence_start\n","                context_end = sentence_end\n","                cur_length = sum(subword_lengths[context_start:context_end])\n","                while True:\n","                    if context_start \u003e 0:\n","                        if cur_length + subword_lengths[context_start - 1] \u003c= max_token_length:\n","                            cur_length += subword_lengths[context_start - 1]\n","                            context_start -= 1\n","                        else:\n","                            break\n","                    if context_end \u003c len(words):\n","                        if cur_length + subword_lengths[context_end] \u003c= max_token_length:\n","                            cur_length += subword_lengths[context_end]\n","                            context_end += 1\n","                        else:\n","                            break\n","\n","            text = \"\"\n","            for word in words[context_start:sentence_start]:\n","                if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n","                    text = text.rstrip()\n","                text += word\n","                text += \" \"\n","\n","            sentence_words = words[sentence_start:sentence_end]\n","            sentence_subword_lengths = subword_lengths[sentence_start:sentence_end]\n","\n","            word_start_char_positions = []\n","            word_end_char_positions = []\n","            for word in sentence_words:\n","                if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n","                    text = text.rstrip()\n","                word_start_char_positions.append(len(text))\n","                text += word\n","                word_end_char_positions.append(len(text))\n","                text += \" \"\n","\n","            for word in words[sentence_end:context_end]:\n","                if word[0] == \"'\" or (len(word) == 1 and is_punctuation(word)):\n","                    text = text.rstrip()\n","                text += word\n","                text += \" \"\n","            text = text.rstrip()\n","\n","            entity_spans = []\n","            original_word_spans = []\n","            for word_start in range(len(sentence_words)):\n","                for word_end in range(word_start, len(sentence_words)):\n","                    if sum(sentence_subword_lengths[word_start:word_end]) \u003c= max_mention_length:\n","                        entity_spans.append(\n","                            (word_start_char_positions[word_start], word_end_char_positions[word_end])\n","                        )\n","                        original_word_spans.append(\n","                            (word_start, word_end + 1)\n","                        )\n","\n","            examples.append(dict(\n","                text=text,\n","                words=sentence_words,\n","                entity_spans=entity_spans,\n","                original_word_spans=original_word_spans,\n","            ))\n","\n","    return examples\n","\n","def is_punctuation(char):\n","    cp = ord(char)\n","    if (cp \u003e= 33 and cp \u003c= 47) or (cp \u003e= 58 and cp \u003c= 64) or (cp \u003e= 91 and cp \u003c= 96) or (cp \u003e= 123 and cp \u003c= 126):\n","        return True\n","    cat = unicodedata.category(char)\n","    if cat.startswith(\"P\"):\n","        return True\n","    return False\n","\n","# Load and process documents\n","test_documents = load_documents(\"eng.testb\")\n","test_examples = load_examples(test_documents)\n","\n","batch_size = 2\n","all_logits = []\n","\n","for batch_start_idx in trange(0, len(test_examples), batch_size):\n","    batch_examples = test_examples[batch_start_idx:batch_start_idx + batch_size]\n","    texts = [example[\"text\"] for example in batch_examples]\n","    entity_spans = [example[\"entity_spans\"] for example in batch_examples]\n","\n","    inputs = tokenizer(texts, entity_spans=entity_spans, return_tensors=\"pt\", padding=True)\n","    inputs = inputs.to(\"cpu\")\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","    all_logits.extend(outputs.logits.tolist())\n","\n","final_labels = [label for document in test_documents for label in document[\"labels\"]]\n","\n","final_predictions = []\n","for example_index, example in enumerate(test_examples):\n","    logits = all_logits[example_index]\n","    max_logits = np.max(logits, axis=1)\n","    max_indices = np.argmax(logits, axis=1)\n","    original_spans = example[\"original_word_spans\"]\n","    predictions = []\n","    for logit, index, span in zip(max_logits, max_indices, original_spans):\n","        if index != 0:\n","            predictions.append((logit, span, model.config.id2label[index]))\n","\n","    predicted_sequence = [\"O\"] * len(example[\"words\"])\n","    for _, span, label in sorted(predictions, key=lambda o: o[0], reverse=True):\n","        if all([o == \"O\" for o in predicted_sequence[span[0] : span[1]]]):\n","            predicted_sequence[span[0]] = \"B-\" + label\n","            if span[1] - span[0] \u003e 1:\n","                predicted_sequence[span[0] + 1 : span[1]] = [\"I-\" + label] * (span[1] - span[0] - 1)\n","\n","    final_predictions += predicted_sequence\n","\n","print(seqeval.metrics.classification_report([final_labels], [final_predictions], digits=4))\n","\n","def predict_user_input(text):\n","    nlp = spacy.load(\"en_core_web_sm\")\n","    doc = nlp(text)\n","\n","    entity_spans = []\n","    original_word_spans = []\n","    for token_start in doc:\n","        for token_end in doc[token_start.i:]:\n","            entity_spans.append((token_start.idx, token_end.idx + len(token_end)))\n","            original_word_spans.append((token_start.i, token_end.i + 1))\n","\n","    inputs = tokenizer(text, entity_spans=entity_spans, return_tensors=\"pt\", padding=True)\n","    inputs = inputs.to(\"cpu\")\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","\n","    logits = outputs.logits\n","    max_logits, max_indices = logits[0].max(dim=1)\n","\n","    predictions = []\n","    for logit, index, span in zip(max_logits, max_indices, original_word_spans):\n","        if index != 0:\n","            predictions.append((logit, span, model.config.id2label[int(index)]))\n","\n","    predicted_sequence = [\"O\"] * len(doc)\n","    for _, span, label in sorted(predictions, key=lambda o: o[0], reverse=True):\n","        if all([o == \"O\" for o in predicted_sequence[span[0] : span[1]]]):\n","            predicted_sequence[span[0]] = \"B-\" + label\n","            if span[1] - span[0] \u003e 1:\n","                predicted_sequence[span[0] + 1 : span[1]] = [\"I-\" + label] * (span[1] - span[0] - 1)\n","\n","    for token, label in zip(doc, predicted_sequence):\n","        print(f\"{token.text}\\t{label}\")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"rOOKNvEn4Bc7"},"outputs":[],"source":["# Take user input and predict\n","user_text = input(\"Enter a sentence: \")\n","predict_user_input(user_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3KsMtToMwlKK"},"outputs":[],"source":["import pickle\n","from transformers import LukeTokenizer, LukeForEntitySpanClassification\n","\n","# Load the model and tokenizer\n","model = LukeForEntitySpanClassification.from_pretrained(\"studio-ousia/luke-large-finetuned-conll-2003\")\n","tokenizer = LukeTokenizer.from_pretrained(\"studio-ousia/luke-large-finetuned-conll-2003\")\n","\n","# Save the model state dictionary\n","model.save_pretrained(\"luke_model\")\n","# Save the tokenizer\n","tokenizer.save_pretrained(\"luke_tokenizer\")\n","\n","print(\"Model and tokenizer saved.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7dBPTG85xODB"},"outputs":[],"source":["import os\n","import unicodedata\n","from flask import Flask, request, jsonify\n","import torch\n","from transformers import LukeTokenizer, LukeForEntitySpanClassification\n","\n","app = Flask(__name__)\n","\n","# Load the model and tokenizer\n","model = LukeForEntitySpanClassification.from_pretrained(\"luke_model\")\n","tokenizer = LukeTokenizer.from_pretrained(\"luke_tokenizer\")\n","model.eval()\n","model.to(\"cuda\")\n","\n","def is_punctuation(char):\n","    cp = ord(char)\n","    if (cp \u003e= 33 and cp \u003c= 47) or (cp \u003e= 58 and cp \u003c= 64) or (cp \u003e= 91 and cp \u003c= 96) or (cp \u003e= 123 and cp \u003c= 126):\n","        return True\n","    cat = unicodedata.category(char)\n","    if cat.startswith(\"P\"):\n","        return True\n","    return False\n","\n","def predict_user_input(text):\n","    nlp = spacy.load(\"en_core_web_sm\")\n","    doc = nlp(text)\n","\n","    entity_spans = []\n","    original_word_spans = []\n","    for token_start in doc:\n","        for token_end in doc[token_start.i:]:\n","            entity_spans.append((token_start.idx, token_end.idx + len(token_end)))\n","            original_word_spans.append((token_start.i, token_end.i + 1))\n","\n","    inputs = tokenizer(text, entity_spans=entity_spans, return_tensors=\"pt\", padding=True)\n","    inputs = inputs.to(\"cuda\")\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","\n","    logits = outputs.logits\n","    max_logits, max_indices = logits[0].max(dim=1)\n","\n","    predictions = []\n","    for logit, index, span in zip(max_logits, max_indices, original_word_spans):\n","        if index != 0:\n","            predictions.append((logit, span, model.config.id2label[int(index)]))\n","\n","    predicted_sequence = [\"O\"] * len(doc)\n","    for _, span, label in sorted(predictions, key=lambda o: o[0], reverse=True):\n","        if all([o == \"O\" for o in predicted_sequence[span[0] : span[1]]]):\n","            predicted_sequence[span[0]] = \"B-\" + label\n","            if span[1] - span[0] \u003e 1:\n","                predicted_sequence[span[0] + 1 : span[1]] = [\"I-\" + label] * (span[1] - span[0] - 1)\n","\n","    result = [{\"word\": token.text, \"label\": label} for token, label in zip(doc, predicted_sequence)]\n","    return result\n","\n","@app.route('/predict', methods=['POST'])\n","def predict():\n","    data = request.get_json()\n","    text = data.get('text', '')\n","    if not text:\n","        return jsonify({\"error\": \"No text provided\"}), 400\n","\n","    result = predict_user_input(text)\n","    return jsonify(result)\n","\n","if __name__ == '__main__':\n","    app.run(debug=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Cmf7_ei67vdR"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNOQkedrTdC2Yce7y0BUASS","gpuType":"T4","mount_file_id":"12igM7PHy7RvfGUJCnxcJS8_WqJsfRnPB","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1c8e28e71b39495dbc45348a95f367aa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_efa5b00b458f406badcc1f9faaaed92b","placeholder":"​","style":"IPY_MODEL_78e223c0f6e9439381d17a982f577615","value":"config.json: 100%"}},"220761fc70d24eb6968c4c1d0f5a6135":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"24f96111434d42d09c90af8fee52b281":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2ca9a054f6eb454ba59a28bb42d75fde":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8c221a49f8842899f80907a07474c01","placeholder":"​","style":"IPY_MODEL_f5e993d244bb450390c91c0dcde55874","value":" 877/877 [00:00\u0026lt;00:00, 25.7kB/s]"}},"315d8c1703d74c60af63b2ee9d4d737e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4da7583fcf5d446182005064c34549b7","IPY_MODEL_d6f32e37106c43e284099840464b4087","IPY_MODEL_636345aab9664ef997d52b66dd209692"],"layout":"IPY_MODEL_655c4956b47a4b1891a1cd92ad0b0b15"}},"38b93d0b3bfc4700b41690eb33c59d3b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4da7583fcf5d446182005064c34549b7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb43ea88b1de43089f796dab7eddf379","placeholder":"​","style":"IPY_MODEL_38b93d0b3bfc4700b41690eb33c59d3b","value":"pytorch_model.bin:  99%"}},"636345aab9664ef997d52b66dd209692":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dfa880357c4c4de4b521d8a293f1979a","placeholder":"​","style":"IPY_MODEL_c36b2f48ace941c78482efc54a3fa915","value":" 2.21G/2.24G [07:18\u0026lt;00:19, 1.34MB/s]"}},"655c4956b47a4b1891a1cd92ad0b0b15":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78e223c0f6e9439381d17a982f577615":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"81e9a6483d644ec690b9920e219d9c34":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_becb19683e8b41d19c3859fe293dd198","max":877,"min":0,"orientation":"horizontal","style":"IPY_MODEL_220761fc70d24eb6968c4c1d0f5a6135","value":877}},"8d0c7adee2644aeeb48b33669be7063c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"becb19683e8b41d19c3859fe293dd198":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c36b2f48ace941c78482efc54a3fa915":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb43ea88b1de43089f796dab7eddf379":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6f32e37106c43e284099840464b4087":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d0c7adee2644aeeb48b33669be7063c","max":2239184431,"min":0,"orientation":"horizontal","style":"IPY_MODEL_24f96111434d42d09c90af8fee52b281","value":2212495360}},"dfa880357c4c4de4b521d8a293f1979a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8c221a49f8842899f80907a07474c01":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec354a015b364ec0bda75d533cab8e48":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1c8e28e71b39495dbc45348a95f367aa","IPY_MODEL_81e9a6483d644ec690b9920e219d9c34","IPY_MODEL_2ca9a054f6eb454ba59a28bb42d75fde"],"layout":"IPY_MODEL_f00d514eadf146ba932d2f63edfe5adc"}},"efa5b00b458f406badcc1f9faaaed92b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f00d514eadf146ba932d2f63edfe5adc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5e993d244bb450390c91c0dcde55874":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}